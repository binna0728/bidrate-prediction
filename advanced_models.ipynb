{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ì‚¬ì •ìœ¨ ì˜ˆì¸¡ ëª¨ë¸\n",
    "\n",
    "## ì‚¬ìš© ëª¨ë¸:\n",
    "1. **ì „í†µ ML**: Random Forest, XGBoost, LightGBM, CatBoost\n",
    "2. **ì•™ìƒë¸”**: Voting, Stacking\n",
    "3. **ë”¥ëŸ¬ë‹**: DNN, LSTM, Transformer\n",
    "4. **AutoML**: AutoGluon, H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install xgboost lightgbm catboost -q\n",
    "!pip install tensorflow keras -q\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn -q\n",
    "!pip install optuna -q  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "!pip install shap -q  # ëª¨ë¸ í•´ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# ì „í†µ ML ëª¨ë¸\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# ìµœì‹  ë¶€ìŠ¤íŒ… ëª¨ë¸\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# ë”¥ëŸ¬ë‹\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"TensorFlow ë²„ì „: {tf.__version__}\")\n",
    "print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_excel('ê°•ì›ë„ë°ê²½ê¸°ì¼ë¶€.xlsx')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "\n",
    "# í•„ìš”í•œ ì»¬ëŸ¼ ì„ íƒ (100ë²ˆì´ë™, 10ë²ˆì´ë™, 3ë²ˆì´ë™ â†’ ì‚¬ì •ìœ¨)\n",
    "X_columns = df.columns[[3, 4, 5]]  # 100ë²ˆì´ë™, 10ë²ˆì´ë™, 3ë²ˆì´ë™\n",
    "y_column = df.columns[1]  # ì‚¬ì •ìœ¨\n",
    "\n",
    "# ë°ì´í„° ì •ì œ\n",
    "data = df[list(X_columns) + [y_column]].dropna()\n",
    "X = data[list(X_columns)]\n",
    "y = data[y_column]\n",
    "\n",
    "print(f\"\\nì •ì œ í›„ ë°ì´í„°: {X.shape}\")\n",
    "print(f\"íŠ¹ì„±: {list(X.columns)}\")\n",
    "print(f\"íƒ€ê²Ÿ: {y_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ì—¬ëŸ¬ ìŠ¤ì¼€ì¼ëŸ¬ ì¤€ë¹„\n",
    "scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'robust': RobustScaler(),\n",
    "    'minmax': MinMaxScaler()\n",
    "}\n",
    "\n",
    "# Standard Scaler ì‚¬ìš©\n",
    "scaler = scalers['standard']\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {X_train.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì „í†µ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ëŸ¬ ML ëª¨ë¸ ì •ì˜\n",
    "ml_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.01),\n",
    "    'ElasticNet': ElasticNet(alpha=0.01),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', gamma='scale'),\n",
    "    'KNN': KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# ëª¨ë¸ í•™ìŠµ ë° í‰ê°€\n",
    "results = {}\n",
    "\n",
    "for name, model in ml_models.items():\n",
    "    # í•™ìŠµ\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # ì˜ˆì¸¡\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # í‰ê°€\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # êµì°¨ ê²€ì¦\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    results[name] = {\n",
    "        'R2': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'CV_Mean': cv_scores.mean(),\n",
    "        'CV_Std': cv_scores.std(),\n",
    "        'Model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:20s}: RÂ²={r2:.4f}, RMSE={rmse:.4f}, CV={cv_scores.mean():.4f}Â±{cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ìµœì‹  ë¶€ìŠ¤íŒ… ëª¨ë¸ (XGBoost, LightGBM, CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "print(f\"XGBoost RÂ² Score: {xgb_r2:.4f}\")\n",
    "\n",
    "results['XGBoost'] = {\n",
    "    'R2': xgb_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, xgb_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, xgb_pred),\n",
    "    'Model': xgb_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test_scaled)\n",
    "lgb_r2 = r2_score(y_test, lgb_pred)\n",
    "\n",
    "print(f\"LightGBM RÂ² Score: {lgb_r2:.4f}\")\n",
    "\n",
    "results['LightGBM'] = {\n",
    "    'R2': lgb_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, lgb_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, lgb_pred),\n",
    "    'Model': lgb_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=200,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_model.fit(X_train_scaled, y_train)\n",
    "cat_pred = cat_model.predict(X_test_scaled)\n",
    "cat_r2 = r2_score(y_test, cat_pred)\n",
    "\n",
    "print(f\"CatBoost RÂ² Score: {cat_r2:.4f}\")\n",
    "\n",
    "results['CatBoost'] = {\n",
    "    'R2': cat_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, cat_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, cat_pred),\n",
    "    'Model': cat_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ë”¥ëŸ¬ë‹ ëª¨ë¸ (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network\n",
    "def create_dnn_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)  # íšŒê·€ ì¶œë ¥\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ëª¨ë¸ ìƒì„±\n",
    "dnn_model = create_dnn_model(X_train_scaled.shape[1])\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN í•™ìŠµ\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "history = dnn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# DNN í‰ê°€\n",
    "dnn_pred = dnn_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "dnn_r2 = r2_score(y_test, dnn_pred)\n",
    "\n",
    "print(f\"DNN RÂ² Score: {dnn_r2:.4f}\")\n",
    "\n",
    "results['DNN'] = {\n",
    "    'R2': dnn_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, dnn_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, dnn_pred),\n",
    "    'Model': dnn_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ íˆìŠ¤í† ë¦¬ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('DNN í•™ìŠµ ì†ì‹¤')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('DNN MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LSTM ì‹œê³„ì—´ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTMì„ ìœ„í•œ ë°ì´í„° ì¬êµ¬ì„± (3D)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# LSTM ëª¨ë¸\n",
    "lstm_model = keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, input_shape=(1, X_train_scaled.shape[1])),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# LSTM í•™ìŠµ\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# LSTM í‰ê°€\n",
    "lstm_pred = lstm_model.predict(X_test_lstm, verbose=0).flatten()\n",
    "lstm_r2 = r2_score(y_test, lstm_pred)\n",
    "\n",
    "print(f\"LSTM RÂ² Score: {lstm_r2:.4f}\")\n",
    "\n",
    "results['LSTM'] = {\n",
    "    'R2': lstm_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, lstm_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, lstm_pred),\n",
    "    'Model': lstm_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ì•™ìƒë¸” ëª¨ë¸ (Voting & Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "\n",
    "# ìƒìœ„ 3ê°œ ëª¨ë¸ ì„ íƒ\n",
    "top_models = sorted(results.items(), key=lambda x: x[1]['R2'], reverse=True)[:3]\n",
    "print(\"ìƒìœ„ 3ê°œ ëª¨ë¸:\")\n",
    "for name, metrics in top_models:\n",
    "    print(f\"  - {name}: RÂ²={metrics['R2']:.4f}\")\n",
    "\n",
    "# Voting ì•™ìƒë¸”\n",
    "voting_models = [(name, metrics['Model']) for name, metrics in top_models if name not in ['DNN', 'LSTM']]\n",
    "\n",
    "if len(voting_models) >= 2:\n",
    "    voting_reg = VotingRegressor(voting_models)\n",
    "    voting_reg.fit(X_train_scaled, y_train)\n",
    "    voting_pred = voting_reg.predict(X_test_scaled)\n",
    "    voting_r2 = r2_score(y_test, voting_pred)\n",
    "    \n",
    "    print(f\"\\nVoting Ensemble RÂ² Score: {voting_r2:.4f}\")\n",
    "    \n",
    "    results['Voting Ensemble'] = {\n",
    "        'R2': voting_r2,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, voting_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, voting_pred),\n",
    "        'Model': voting_reg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking ì•™ìƒë¸”\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=100, random_state=42)),\n",
    "    ('lgb', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))\n",
    "]\n",
    "\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "stacking_reg.fit(X_train_scaled, y_train)\n",
    "stacking_pred = stacking_reg.predict(X_test_scaled)\n",
    "stacking_r2 = r2_score(y_test, stacking_pred)\n",
    "\n",
    "print(f\"Stacking Ensemble RÂ² Score: {stacking_r2:.4f}\")\n",
    "\n",
    "results['Stacking Ensemble'] = {\n",
    "    'R2': stacking_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, stacking_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, stacking_pred),\n",
    "    'Model': stacking_reg\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    # êµì°¨ ê²€ì¦\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Optuna ìµœì í™”\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# ìµœì  íŒŒë¼ë¯¸í„°\n",
    "best_params = study.best_params\n",
    "print(f\"\\nìµœì  íŒŒë¼ë¯¸í„°:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ\n",
    "optimized_xgb = xgb.XGBRegressor(**best_params, random_state=42)\n",
    "optimized_xgb.fit(X_train_scaled, y_train)\n",
    "opt_pred = optimized_xgb.predict(X_test_scaled)\n",
    "opt_r2 = r2_score(y_test, opt_pred)\n",
    "\n",
    "print(f\"\\nOptimized XGBoost RÂ² Score: {opt_r2:.4f}\")\n",
    "\n",
    "results['Optimized XGBoost'] = {\n",
    "    'R2': opt_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, opt_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, opt_pred),\n",
    "    'Model': optimized_xgb\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. ëª¨ë¸ ë¹„êµ ë° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì •ë¦¬\n",
    "results_df = pd.DataFrame(results).T[['R2', 'RMSE', 'MAE']]\n",
    "results_df = results_df.sort_values('R2', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\"*60)\n",
    "print(results_df)\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸\n",
    "best_model_name = results_df.index[0]\n",
    "best_r2 = results_df.iloc[0]['R2']\n",
    "print(f\"\\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model_name} (RÂ²={best_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# RÂ² Score\n",
    "axes[0].barh(results_df.index, results_df['R2'])\n",
    "axes[0].set_xlabel('RÂ² Score')\n",
    "axes[0].set_title('ëª¨ë¸ë³„ RÂ² Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1].barh(results_df.index, results_df['RMSE'], color='orange')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_title('ëª¨ë¸ë³„ RMSE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[2].barh(results_df.index, results_df['MAE'], color='green')\n",
    "axes[2].set_xlabel('MAE')\n",
    "axes[2].set_title('ëª¨ë¸ë³„ MAE (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœê³  ëª¨ë¸ì˜ ì˜ˆì¸¡ ì‹œê°í™”\n",
    "best_model = results[best_model_name]['Model']\n",
    "\n",
    "# ì˜ˆì¸¡\n",
    "if best_model_name in ['DNN', 'LSTM']:\n",
    "    if best_model_name == 'LSTM':\n",
    "        best_pred = best_model.predict(X_test_lstm, verbose=0).flatten()\n",
    "    else:\n",
    "        best_pred = best_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "else:\n",
    "    best_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# ì‹¤ì œ vs ì˜ˆì¸¡\n",
    "axes[0].scatter(y_test, best_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('ì‹¤ì œ ì‚¬ì •ìœ¨')\n",
    "axes[0].set_ylabel('ì˜ˆì¸¡ ì‚¬ì •ìœ¨')\n",
    "axes[0].set_title(f'{best_model_name} - ì‹¤ì œ vs ì˜ˆì¸¡ (RÂ²={best_r2:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# ì”ì°¨ í”Œë¡¯\n",
    "residuals = y_test - best_pred\n",
    "axes[1].scatter(best_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('ì˜ˆì¸¡ ì‚¬ì •ìœ¨')\n",
    "axes[1].set_ylabel('ì”ì°¨')\n",
    "axes[1].set_title(f'{best_model_name} - ì”ì°¨ í”Œë¡¯')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ë§Œ)\n",
    "tree_models = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Gradient Boosting']\n",
    "\n",
    "for name in tree_models:\n",
    "    if name in results:\n",
    "        model = results[name]['Model']\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.bar(X.columns, importances)\n",
    "            plt.xlabel('íŠ¹ì„±')\n",
    "            plt.ylabel('ì¤‘ìš”ë„')\n",
    "            plt.title(f'{name} - Feature Importance')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"{name} Feature Importance:\")\n",
    "            for col, imp in zip(X.columns, importances):\n",
    "                print(f\"  {col}: {imp:.4f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ìµœì¢… ëª¨ë¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# ë”¥ëŸ¬ë‹ ëª¨ë¸ì¸ ê²½ìš°\n",
    "if best_model_name in ['DNN', 'LSTM']:\n",
    "    best_model.save(f'models/best_{best_model_name.lower()}_model.h5')\n",
    "    print(f\"ë”¥ëŸ¬ë‹ ëª¨ë¸ ì €ì¥: models/best_{best_model_name.lower()}_model.h5\")\n",
    "else:\n",
    "    # ì¼ë°˜ ML ëª¨ë¸\n",
    "    model_data = {\n",
    "        'model': best_model,\n",
    "        'scaler': scaler,\n",
    "        'features': list(X.columns),\n",
    "        'model_type': best_model_name,\n",
    "        'metrics': {\n",
    "            'r2': best_r2,\n",
    "            'rmse': results[best_model_name]['RMSE'],\n",
    "            'mae': results[best_model_name]['MAE']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, 'models/best_model.pkl')\n",
    "    print(f\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: models/best_model.pkl\")\n",
    "\n",
    "# ëª¨ë“  ê²°ê³¼ ì €ì¥\n",
    "results_df.to_csv('model_comparison_results.csv')\n",
    "print(\"\\nëª¨ë“  ê²°ê³¼ ì €ì¥: model_comparison_results.csv\")\n",
    "\n",
    "print(f\"\\nìµœì¢… ì„ íƒ ëª¨ë¸: {best_model_name}\")\n",
    "print(f\"ì„±ëŠ¥: RÂ²={best_r2:.4f}, RMSE={results[best_model_name]['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. ê²°ë¡  ë° ì¶”ì²œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ìµœì¢… ë¶„ì„ ê²°ê³¼\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Top 5 ëª¨ë¸\n",
    "print(\"\\nğŸ† Top 5 ëª¨ë¸:\")\n",
    "for i, (idx, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
    "    print(f\"{i}. {idx:20s}: RÂ²={row['R2']:.4f}, RMSE={row['RMSE']:.4f}\")\n",
    "\n",
    "# ëª¨ë¸ë³„ íŠ¹ì§•\n",
    "print(\"\\nğŸ“Š ëª¨ë¸ë³„ íŠ¹ì§•:\")\n",
    "if 'XGBoost' in results_df.index[:3]:\n",
    "    print(\"  - XGBoost: ê· í˜• ì¡íŒ ì„±ëŠ¥, ë¹ ë¥¸ ì†ë„\")\n",
    "if 'LightGBM' in results_df.index[:3]:\n",
    "    print(\"  - LightGBM: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ìµœì í™”\")\n",
    "if 'CatBoost' in results_df.index[:3]:\n",
    "    print(\"  - CatBoost: ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ ìš°ìˆ˜\")\n",
    "if 'DNN' in results_df.index[:3]:\n",
    "    print(\"  - DNN: ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ\")\n",
    "if 'Stacking Ensemble' in results_df.index[:3]:\n",
    "    print(\"  - Stacking: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì¥ì  ê²°í•©\")\n",
    "\n",
    "# ì¶”ì²œ\n",
    "print(\"\\nğŸ’¡ ì¶”ì²œ:\")\n",
    "if best_r2 > 0.8:\n",
    "    print(f\"  âœ… {best_model_name} ëª¨ë¸ ì‚¬ìš© ê¶Œì¥ (ìš°ìˆ˜í•œ ì„±ëŠ¥)\")\n",
    "elif best_r2 > 0.6:\n",
    "    print(f\"  âš ï¸ {best_model_name} ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥ (ì¶”ê°€ íŠ¹ì„± í•„ìš”í•  ìˆ˜ ìˆìŒ)\")\n",
    "else:\n",
    "    print(f\"  âŒ ë” ë§ì€ ë°ì´í„°ë‚˜ íŠ¹ì„±ì´ í•„ìš”í•¨\")\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ì˜ˆì¸¡ ì •í™•ë„: {best_r2*100:.1f}%\")\n",
    "print(f\"ğŸ“‰ í‰ê·  ì˜¤ì°¨: Â±{results[best_model_name]['MAE']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}