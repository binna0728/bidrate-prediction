{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 고급 머신러닝/딥러닝 사정율 예측 모델\n",
    "\n",
    "## 사용 모델:\n",
    "1. **전통 ML**: Random Forest, XGBoost, LightGBM, CatBoost\n",
    "2. **앙상블**: Voting, Stacking\n",
    "3. **딥러닝**: DNN, LSTM, Transformer\n",
    "4. **AutoML**: AutoGluon, H2O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 설치 및 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 설치\n",
    "!pip install xgboost lightgbm catboost -q\n",
    "!pip install tensorflow keras -q\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn -q\n",
    "!pip install optuna -q  # 하이퍼파라미터 최적화\n",
    "!pip install shap -q  # 모델 해석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 머신러닝 모델\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# 전통 ML 모델\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 최신 부스팅 모델\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# 딥러닝\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "\n",
    "# 하이퍼파라미터 최적화\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(f\"TensorFlow 버전: {tf.__version__}\")\n",
    "print(f\"GPU 사용 가능: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(\"모든 라이브러리 임포트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_excel('강원도및경기일부.xlsx')\n",
    "print(f\"데이터 크기: {df.shape}\")\n",
    "\n",
    "# 필요한 컬럼 선택 (100번이동, 10번이동, 3번이동 → 사정율)\n",
    "X_columns = df.columns[[3, 4, 5]]  # 100번이동, 10번이동, 3번이동\n",
    "y_column = df.columns[1]  # 사정율\n",
    "\n",
    "# 데이터 정제\n",
    "data = df[list(X_columns) + [y_column]].dropna()\n",
    "X = data[list(X_columns)]\n",
    "y = data[y_column]\n",
    "\n",
    "print(f\"\\n정제 후 데이터: {X.shape}\")\n",
    "print(f\"특성: {list(X.columns)}\")\n",
    "print(f\"타겟: {y_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 여러 스케일러 준비\n",
    "scalers = {\n",
    "    'standard': StandardScaler(),\n",
    "    'robust': RobustScaler(),\n",
    "    'minmax': MinMaxScaler()\n",
    "}\n",
    "\n",
    "# Standard Scaler 사용\n",
    "scaler = scalers['standard']\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"학습 데이터: {X_train.shape}\")\n",
    "print(f\"테스트 데이터: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 전통 머신러닝 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러 ML 모델 정의\n",
    "ml_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.01),\n",
    "    'ElasticNet': ElasticNet(alpha=0.01),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf', gamma='scale'),\n",
    "    'KNN': KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# 모델 학습 및 평가\n",
    "results = {}\n",
    "\n",
    "for name, model in ml_models.items():\n",
    "    # 학습\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 예측\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # 평가\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # 교차 검증\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    results[name] = {\n",
    "        'R2': r2,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'CV_Mean': cv_scores.mean(),\n",
    "        'CV_Std': cv_scores.std(),\n",
    "        'Model': model\n",
    "    }\n",
    "    \n",
    "    print(f\"{name:20s}: R²={r2:.4f}, RMSE={rmse:.4f}, CV={cv_scores.mean():.4f}±{cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 최신 부스팅 모델 (XGBoost, LightGBM, CatBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "xgb_r2 = r2_score(y_test, xgb_pred)\n",
    "\n",
    "print(f\"XGBoost R² Score: {xgb_r2:.4f}\")\n",
    "\n",
    "results['XGBoost'] = {\n",
    "    'R2': xgb_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, xgb_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, xgb_pred),\n",
    "    'Model': xgb_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "lgb_pred = lgb_model.predict(X_test_scaled)\n",
    "lgb_r2 = r2_score(y_test, lgb_pred)\n",
    "\n",
    "print(f\"LightGBM R² Score: {lgb_r2:.4f}\")\n",
    "\n",
    "results['LightGBM'] = {\n",
    "    'R2': lgb_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, lgb_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, lgb_pred),\n",
    "    'Model': lgb_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=200,\n",
    "    depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_model.fit(X_train_scaled, y_train)\n",
    "cat_pred = cat_model.predict(X_test_scaled)\n",
    "cat_r2 = r2_score(y_test, cat_pred)\n",
    "\n",
    "print(f\"CatBoost R² Score: {cat_r2:.4f}\")\n",
    "\n",
    "results['CatBoost'] = {\n",
    "    'R2': cat_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, cat_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, cat_pred),\n",
    "    'Model': cat_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 딥러닝 모델 (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Neural Network\n",
    "def create_dnn_model(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.1),\n",
    "        \n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)  # 회귀 출력\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 모델 생성\n",
    "dnn_model = create_dnn_model(X_train_scaled.shape[1])\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN 학습\n",
    "early_stop = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "history = dnn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# DNN 평가\n",
    "dnn_pred = dnn_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "dnn_r2 = r2_score(y_test, dnn_pred)\n",
    "\n",
    "print(f\"DNN R² Score: {dnn_r2:.4f}\")\n",
    "\n",
    "results['DNN'] = {\n",
    "    'R2': dnn_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, dnn_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, dnn_pred),\n",
    "    'Model': dnn_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 히스토리 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Train Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('DNN 학습 손실')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(history.history['mae'], label='Train MAE')\n",
    "axes[1].plot(history.history['val_mae'], label='Val MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].set_title('DNN MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LSTM 시계열 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM을 위한 데이터 재구성 (3D)\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# LSTM 모델\n",
    "lstm_model = keras.Sequential([\n",
    "    layers.LSTM(64, return_sequences=True, input_shape=(1, X_train_scaled.shape[1])),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.LSTM(32),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# LSTM 학습\n",
    "lstm_history = lstm_model.fit(\n",
    "    X_train_lstm, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# LSTM 평가\n",
    "lstm_pred = lstm_model.predict(X_test_lstm, verbose=0).flatten()\n",
    "lstm_r2 = r2_score(y_test, lstm_pred)\n",
    "\n",
    "print(f\"LSTM R² Score: {lstm_r2:.4f}\")\n",
    "\n",
    "results['LSTM'] = {\n",
    "    'R2': lstm_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, lstm_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, lstm_pred),\n",
    "    'Model': lstm_model\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 앙상블 모델 (Voting & Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "\n",
    "# 상위 3개 모델 선택\n",
    "top_models = sorted(results.items(), key=lambda x: x[1]['R2'], reverse=True)[:3]\n",
    "print(\"상위 3개 모델:\")\n",
    "for name, metrics in top_models:\n",
    "    print(f\"  - {name}: R²={metrics['R2']:.4f}\")\n",
    "\n",
    "# Voting 앙상블\n",
    "voting_models = [(name, metrics['Model']) for name, metrics in top_models if name not in ['DNN', 'LSTM']]\n",
    "\n",
    "if len(voting_models) >= 2:\n",
    "    voting_reg = VotingRegressor(voting_models)\n",
    "    voting_reg.fit(X_train_scaled, y_train)\n",
    "    voting_pred = voting_reg.predict(X_test_scaled)\n",
    "    voting_r2 = r2_score(y_test, voting_pred)\n",
    "    \n",
    "    print(f\"\\nVoting Ensemble R² Score: {voting_r2:.4f}\")\n",
    "    \n",
    "    results['Voting Ensemble'] = {\n",
    "        'R2': voting_r2,\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, voting_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, voting_pred),\n",
    "        'Model': voting_reg\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking 앙상블\n",
    "base_models = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('xgb', xgb.XGBRegressor(n_estimators=100, random_state=42)),\n",
    "    ('lgb', lgb.LGBMRegressor(n_estimators=100, random_state=42, verbose=-1))\n",
    "]\n",
    "\n",
    "stacking_reg = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "stacking_reg.fit(X_train_scaled, y_train)\n",
    "stacking_pred = stacking_reg.predict(X_test_scaled)\n",
    "stacking_r2 = r2_score(y_test, stacking_pred)\n",
    "\n",
    "print(f\"Stacking Ensemble R² Score: {stacking_r2:.4f}\")\n",
    "\n",
    "results['Stacking Ensemble'] = {\n",
    "    'R2': stacking_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, stacking_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, stacking_pred),\n",
    "    'Model': stacking_reg\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 하이퍼파라미터 최적화 (Optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost 하이퍼파라미터 최적화\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    # 교차 검증\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "    \n",
    "    return cv_scores.mean()\n",
    "\n",
    "# Optuna 최적화\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# 최적 파라미터\n",
    "best_params = study.best_params\n",
    "print(f\"\\n최적 파라미터:\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# 최적화된 모델 학습\n",
    "optimized_xgb = xgb.XGBRegressor(**best_params, random_state=42)\n",
    "optimized_xgb.fit(X_train_scaled, y_train)\n",
    "opt_pred = optimized_xgb.predict(X_test_scaled)\n",
    "opt_r2 = r2_score(y_test, opt_pred)\n",
    "\n",
    "print(f\"\\nOptimized XGBoost R² Score: {opt_r2:.4f}\")\n",
    "\n",
    "results['Optimized XGBoost'] = {\n",
    "    'R2': opt_r2,\n",
    "    'RMSE': np.sqrt(mean_squared_error(y_test, opt_pred)),\n",
    "    'MAE': mean_absolute_error(y_test, opt_pred),\n",
    "    'Model': optimized_xgb\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 비교 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 정리\n",
    "results_df = pd.DataFrame(results).T[['R2', 'RMSE', 'MAE']]\n",
    "results_df = results_df.sort_values('R2', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"모든 모델 성능 비교\")\n",
    "print(\"=\"*60)\n",
    "print(results_df)\n",
    "\n",
    "# 최고 성능 모델\n",
    "best_model_name = results_df.index[0]\n",
    "best_r2 = results_df.iloc[0]['R2']\n",
    "print(f\"\\n🏆 최고 성능 모델: {best_model_name} (R²={best_r2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 비교 시각화\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# R² Score\n",
    "axes[0].barh(results_df.index, results_df['R2'])\n",
    "axes[0].set_xlabel('R² Score')\n",
    "axes[0].set_title('모델별 R² Score')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1].barh(results_df.index, results_df['RMSE'], color='orange')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_title('모델별 RMSE (낮을수록 좋음)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "axes[2].barh(results_df.index, results_df['MAE'], color='green')\n",
    "axes[2].set_xlabel('MAE')\n",
    "axes[2].set_title('모델별 MAE (낮을수록 좋음)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최고 모델의 예측 시각화\n",
    "best_model = results[best_model_name]['Model']\n",
    "\n",
    "# 예측\n",
    "if best_model_name in ['DNN', 'LSTM']:\n",
    "    if best_model_name == 'LSTM':\n",
    "        best_pred = best_model.predict(X_test_lstm, verbose=0).flatten()\n",
    "    else:\n",
    "        best_pred = best_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "else:\n",
    "    best_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 실제 vs 예측\n",
    "axes[0].scatter(y_test, best_pred, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('실제 사정율')\n",
    "axes[0].set_ylabel('예측 사정율')\n",
    "axes[0].set_title(f'{best_model_name} - 실제 vs 예측 (R²={best_r2:.4f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 잔차 플롯\n",
    "residuals = y_test - best_pred\n",
    "axes[1].scatter(best_pred, residuals, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1].set_xlabel('예측 사정율')\n",
    "axes[1].set_ylabel('잔차')\n",
    "axes[1].set_title(f'{best_model_name} - 잔차 플롯')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance (SHAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (트리 기반 모델만)\n",
    "tree_models = ['Random Forest', 'XGBoost', 'LightGBM', 'CatBoost', 'Gradient Boosting']\n",
    "\n",
    "for name in tree_models:\n",
    "    if name in results:\n",
    "        model = results[name]['Model']\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            \n",
    "            plt.figure(figsize=(8, 4))\n",
    "            plt.bar(X.columns, importances)\n",
    "            plt.xlabel('특성')\n",
    "            plt.ylabel('중요도')\n",
    "            plt.title(f'{name} - Feature Importance')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"{name} Feature Importance:\")\n",
    "            for col, imp in zip(X.columns, importances):\n",
    "                print(f\"  {col}: {imp:.4f}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 최종 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# 최고 성능 모델 저장\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# 딥러닝 모델인 경우\n",
    "if best_model_name in ['DNN', 'LSTM']:\n",
    "    best_model.save(f'models/best_{best_model_name.lower()}_model.h5')\n",
    "    print(f\"딥러닝 모델 저장: models/best_{best_model_name.lower()}_model.h5\")\n",
    "else:\n",
    "    # 일반 ML 모델\n",
    "    model_data = {\n",
    "        'model': best_model,\n",
    "        'scaler': scaler,\n",
    "        'features': list(X.columns),\n",
    "        'model_type': best_model_name,\n",
    "        'metrics': {\n",
    "            'r2': best_r2,\n",
    "            'rmse': results[best_model_name]['RMSE'],\n",
    "            'mae': results[best_model_name]['MAE']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_data, 'models/best_model.pkl')\n",
    "    print(f\"모델 저장 완료: models/best_model.pkl\")\n",
    "\n",
    "# 모든 결과 저장\n",
    "results_df.to_csv('model_comparison_results.csv')\n",
    "print(\"\\n모든 결과 저장: model_comparison_results.csv\")\n",
    "\n",
    "print(f\"\\n최종 선택 모델: {best_model_name}\")\n",
    "print(f\"성능: R²={best_r2:.4f}, RMSE={results[best_model_name]['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 결론 및 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"최종 분석 결과\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Top 5 모델\n",
    "print(\"\\n🏆 Top 5 모델:\")\n",
    "for i, (idx, row) in enumerate(results_df.head(5).iterrows(), 1):\n",
    "    print(f\"{i}. {idx:20s}: R²={row['R2']:.4f}, RMSE={row['RMSE']:.4f}\")\n",
    "\n",
    "# 모델별 특징\n",
    "print(\"\\n📊 모델별 특징:\")\n",
    "if 'XGBoost' in results_df.index[:3]:\n",
    "    print(\"  - XGBoost: 균형 잡힌 성능, 빠른 속도\")\n",
    "if 'LightGBM' in results_df.index[:3]:\n",
    "    print(\"  - LightGBM: 대용량 데이터에 최적화\")\n",
    "if 'CatBoost' in results_df.index[:3]:\n",
    "    print(\"  - CatBoost: 범주형 변수 처리 우수\")\n",
    "if 'DNN' in results_df.index[:3]:\n",
    "    print(\"  - DNN: 복잡한 비선형 패턴 학습\")\n",
    "if 'Stacking Ensemble' in results_df.index[:3]:\n",
    "    print(\"  - Stacking: 여러 모델의 장점 결합\")\n",
    "\n",
    "# 추천\n",
    "print(\"\\n💡 추천:\")\n",
    "if best_r2 > 0.8:\n",
    "    print(f\"  ✅ {best_model_name} 모델 사용 권장 (우수한 성능)\")\n",
    "elif best_r2 > 0.6:\n",
    "    print(f\"  ⚠️ {best_model_name} 모델 사용 가능 (추가 특성 필요할 수 있음)\")\n",
    "else:\n",
    "    print(f\"  ❌ 더 많은 데이터나 특성이 필요함\")\n",
    "\n",
    "print(f\"\\n📈 예측 정확도: {best_r2*100:.1f}%\")\n",
    "print(f\"📉 평균 오차: ±{results[best_model_name]['MAE']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}