{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤗 최신 Transformer & Foundation Models for Tabular Data\n",
    "\n",
    "## 사용 모델:\n",
    "1. **TabTransformer** - Google Research\n",
    "2. **FT-Transformer** - Feature Tokenizer Transformer\n",
    "3. **SAINT** - Self-Attention and Intersample Attention\n",
    "4. **TabNet** - Google Cloud AI\n",
    "5. **AutoGluon-Tabular** - AWS AutoML\n",
    "6. **Hugging Face TabularTransformer**\n",
    "7. **XGBoost with Transformers** - Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 최신 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer 기반 Tabular 모델\n",
    "!pip install transformers datasets accelerate -q\n",
    "!pip install pytorch-tabnet -q\n",
    "!pip install autogluon.tabular -q\n",
    "!pip install tab-transformer-pytorch -q\n",
    "!pip install rtdl -q  # Revisiting Deep Learning for Tabular Data\n",
    "!pip install saint-pytorch -q\n",
    "!pip install torch torchvision -q\n",
    "!pip install einops -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformer models\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tab_transformer_pytorch import TabTransformer, FTTransformer\n",
    "from saint_pytorch import SAINT\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# AutoML\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# RTDL (Revisiting Deep Learning)\n",
    "import rtdl\n",
    "\n",
    "# 기본 ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "df = pd.read_excel('강원도및경기일부.xlsx')\n",
    "print(f\"데이터 크기: {df.shape}\")\n",
    "\n",
    "# 특성과 타겟 설정\n",
    "X_columns = df.columns[[3, 4, 5]]  # 100번이동, 10번이동, 3번이동\n",
    "y_column = df.columns[1]  # 사정율\n",
    "\n",
    "# 데이터 정제\n",
    "data = df[list(X_columns) + [y_column]].dropna()\n",
    "X = data[list(X_columns)].values\n",
    "y = data[y_column].values\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"학습 데이터: {X_train.shape}\")\n",
    "print(f\"테스트 데이터: {X_test.shape}\")\n",
    "print(f\"특성 개수: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TabTransformer (Google Research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabTransformer 모델\n",
    "tab_transformer = TabTransformer(\n",
    "    categories=(),  # 연속형 변수만 사용\n",
    "    num_continuous=3,  # 연속형 변수 3개\n",
    "    dim=32,\n",
    "    dim_out=1,  # 회귀 출력\n",
    "    depth=6,  # Transformer 층 수\n",
    "    heads=8,  # Attention heads\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.1,\n",
    "    mlp_hidden_mults=(4, 2),\n",
    "    mlp_act=nn.ReLU(),\n",
    ").to(device)\n",
    "\n",
    "print(\"TabTransformer 아키텍처:\")\n",
    "print(f\"  - Transformer Depth: 6\")\n",
    "print(f\"  - Attention Heads: 8\")\n",
    "print(f\"  - Hidden Dimension: 32\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in tab_transformer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabTransformer 학습\n",
    "def train_transformer_model(model, X_train, y_train, X_test, y_test, epochs=100):\n",
    "    # 텐서 변환\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "    \n",
    "    # 옵티마이저와 손실 함수\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # 학습\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TabTransformer는 범주형과 연속형을 분리해서 입력\n",
    "        outputs = model(None, X_train_tensor)  # (categories, continuous)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Eval\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(None, X_test_tensor)\n",
    "                test_loss = criterion(test_outputs, y_test_tensor)\n",
    "                test_losses.append(test_loss.item())\n",
    "                \n",
    "                if epoch % 20 == 0:\n",
    "                    r2 = r2_score(y_test_tensor.cpu(), test_outputs.cpu())\n",
    "                    print(f\"Epoch {epoch}: Train Loss={loss.item():.4f}, Test R²={r2:.4f}\")\n",
    "    \n",
    "    # 최종 평가\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(None, X_test_tensor).cpu().numpy()\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    \n",
    "    return model, r2, rmse, train_losses\n",
    "\n",
    "# 학습 실행\n",
    "tab_transformer_trained, tab_r2, tab_rmse, tab_losses = train_transformer_model(\n",
    "    tab_transformer, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ TabTransformer 최종 성능: R²={tab_r2:.4f}, RMSE={tab_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FT-Transformer (Feature Tokenizer Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT-Transformer\n",
    "ft_transformer = FTTransformer(\n",
    "    categories=(),  # 범주형 없음\n",
    "    num_continuous=3,\n",
    "    dim=32,\n",
    "    dim_out=1,\n",
    "    depth=3,\n",
    "    heads=8,\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(\"FT-Transformer (Feature Tokenizer):\")\n",
    "print(\"  - 각 특성을 개별 토큰으로 변환\")\n",
    "print(\"  - Self-attention으로 특성 간 상호작용 학습\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in ft_transformer.parameters()):,}\")\n",
    "\n",
    "# FT-Transformer 학습\n",
    "ft_transformer_trained, ft_r2, ft_rmse, ft_losses = train_transformer_model(\n",
    "    ft_transformer, X_train_scaled, y_train, X_test_scaled, y_test, epochs=100\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ FT-Transformer 최종 성능: R²={ft_r2:.4f}, RMSE={ft_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SAINT (Self-Attention and Intersample Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAINT 모델 - Row & Column Attention\n",
    "saint_model = SAINT(\n",
    "    num_features=3,\n",
    "    num_classes=1,  # 회귀이므로 1\n",
    "    dim=32,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    dim_head=16,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(\"SAINT (Self-Attention & Intersample):\")\n",
    "print(\"  - Row-wise attention: 샘플 간 관계 학습\")\n",
    "print(\"  - Column-wise attention: 특성 간 관계 학습\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in saint_model.parameters()):,}\")\n",
    "\n",
    "# SAINT는 배치 단위로 처리\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_scaled),\n",
    "    torch.FloatTensor(y_train)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# SAINT 학습\n",
    "optimizer = optim.AdamW(saint_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "saint_model.train()\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = saint_model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# SAINT 평가\n",
    "saint_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    predictions = saint_model(X_test_tensor).cpu().numpy().squeeze()\n",
    "    saint_r2 = r2_score(y_test, predictions)\n",
    "    saint_rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print(f\"\\n✅ SAINT 최종 성능: R²={saint_r2:.4f}, RMSE={saint_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TabNet (Google Cloud AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNet - Attention 기반 특성 선택\n",
    "tabnet_model = TabNetRegressor(\n",
    "    n_d=8,  # Width of decision prediction layer\n",
    "    n_a=8,  # Width of attention embedding\n",
    "    n_steps=3,  # Number of steps\n",
    "    gamma=1.3,  # Relaxation parameter\n",
    "    cat_idxs=[],  # 범주형 변수 인덱스 (없음)\n",
    "    cat_dims=[],  # 범주형 변수 차원 (없음)\n",
    "    cat_emb_dim=1,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    epsilon=1e-15,\n",
    "    momentum=0.02,\n",
    "    lambda_sparse=1e-3,  # Sparsity\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=0.02),\n",
    "    scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',  # \"sparsemax\" or \"entmax\"\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"TabNet (Explainable DL):\")\n",
    "print(\"  - Sequential attention for feature selection\")\n",
    "print(\"  - Interpretable feature importance\")\n",
    "print(\"  - No preprocessing required\")\n",
    "\n",
    "# TabNet 학습\n",
    "tabnet_model.fit(\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train.reshape(-1, 1),\n",
    "    eval_set=[(X_test_scaled, y_test.reshape(-1, 1))],\n",
    "    eval_metric=['rmse'],\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128\n",
    ")\n",
    "\n",
    "# TabNet 평가\n",
    "tabnet_pred = tabnet_model.predict(X_test_scaled).flatten()\n",
    "tabnet_r2 = r2_score(y_test, tabnet_pred)\n",
    "tabnet_rmse = np.sqrt(mean_squared_error(y_test, tabnet_pred))\n",
    "\n",
    "print(f\"\\n✅ TabNet 최종 성능: R²={tabnet_r2:.4f}, RMSE={tabnet_rmse:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = tabnet_model.feature_importances_\n",
    "print(\"\\nTabNet Feature Importances:\")\n",
    "for i, imp in enumerate(feature_importances):\n",
    "    print(f\"  Feature {i}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RTDL Models (Revisiting Deep Learning for Tabular Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTDL - ResNet for Tabular\n",
    "rtdl_resnet = rtdl.ResNet.make_baseline(\n",
    "    d_in=3,  # 입력 차원\n",
    "    d_out=1,  # 출력 차원 (회귀)\n",
    "    n_blocks=2,\n",
    "    d_main=128,\n",
    "    d_hidden=256,\n",
    "    dropout_first=0.2,\n",
    "    dropout_second=0.1,\n",
    ").to(device)\n",
    "\n",
    "print(\"RTDL ResNet for Tabular:\")\n",
    "print(\"  - Residual connections for tabular data\")\n",
    "print(\"  - Deep architecture without degradation\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in rtdl_resnet.parameters()):,}\")\n",
    "\n",
    "# ResNet 학습\n",
    "optimizer = optim.AdamW(rtdl_resnet.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "rtdl_resnet.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rtdl_resnet(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={loss.item():.4f}\")\n",
    "\n",
    "# ResNet 평가\n",
    "rtdl_resnet.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = rtdl_resnet(X_test_tensor).cpu().numpy().squeeze()\n",
    "    rtdl_r2 = r2_score(y_test, predictions)\n",
    "    rtdl_rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print(f\"\\n✅ RTDL ResNet 최종 성능: R²={rtdl_r2:.4f}, RMSE={rtdl_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. AutoGluon (AWS AutoML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoGluon - 자동 앙상블\n",
    "train_data = pd.DataFrame(X_train, columns=['feat_0', 'feat_1', 'feat_2'])\n",
    "train_data['target'] = y_train\n",
    "test_data = pd.DataFrame(X_test, columns=['feat_0', 'feat_1', 'feat_2'])\n",
    "\n",
    "print(\"AutoGluon TabularPredictor:\")\n",
    "print(\"  - Automatic model selection & ensembling\")\n",
    "print(\"  - Neural networks + Boosted trees + More\")\n",
    "print(\"  - Automatic hyperparameter tuning\")\n",
    "\n",
    "# AutoGluon 학습\n",
    "predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    eval_metric='r2',\n",
    "    verbosity=0\n",
    ").fit(\n",
    "    train_data=train_data,\n",
    "    presets='best_quality',  # 최고 품질 설정\n",
    "    time_limit=120  # 2분 제한\n",
    ")\n",
    "\n",
    "# AutoGluon 평가\n",
    "autogluon_pred = predictor.predict(test_data)\n",
    "autogluon_r2 = r2_score(y_test, autogluon_pred)\n",
    "autogluon_rmse = np.sqrt(mean_squared_error(y_test, autogluon_pred))\n",
    "\n",
    "print(f\"\\n✅ AutoGluon 최종 성능: R²={autogluon_r2:.4f}, RMSE={autogluon_rmse:.4f}\")\n",
    "\n",
    "# 모델 리더보드\n",
    "print(\"\\nAutoGluon Model Leaderboard:\")\n",
    "leaderboard = predictor.leaderboard(test_data)\n",
    "print(leaderboard[['model', 'score_val', 'score_test']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hybrid: XGBoost + Transformer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Transformer로 특성 추출 후 XGBoost\n",
    "print(\"Hybrid Model: Transformer Feature Extractor + XGBoost\")\n",
    "print(\"  - Transformer로 고차원 특성 학습\")\n",
    "print(\"  - XGBoost로 최종 예측\")\n",
    "\n",
    "# TabTransformer에서 특성 추출 (마지막 층 전)\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, transformer_model):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transformer의 중간 representation 추출\n",
    "        with torch.no_grad():\n",
    "            # 여기서는 간단히 원본 특성과 변환된 특성을 연결\n",
    "            transformed = self.transformer(None, x)\n",
    "        return torch.cat([x, transformed], dim=1)\n",
    "\n",
    "# 특성 추출\n",
    "extractor = FeatureExtractor(tab_transformer_trained).to(device)\n",
    "extractor.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_train_enhanced = extractor(torch.FloatTensor(X_train_scaled).to(device)).cpu().numpy()\n",
    "    X_test_enhanced = extractor(torch.FloatTensor(X_test_scaled).to(device)).cpu().numpy()\n",
    "\n",
    "print(f\"Enhanced features shape: {X_train_enhanced.shape}\")\n",
    "\n",
    "# XGBoost with enhanced features\n",
    "xgb_hybrid = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_hybrid.fit(X_train_enhanced, y_train)\n",
    "hybrid_pred = xgb_hybrid.predict(X_test_enhanced)\n",
    "hybrid_r2 = r2_score(y_test, hybrid_pred)\n",
    "hybrid_rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))\n",
    "\n",
    "print(f\"\\n✅ Hybrid (Transformer+XGBoost) 최종 성능: R²={hybrid_r2:.4f}, RMSE={hybrid_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 모든 모델 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 정리\n",
    "results = {\n",
    "    'TabTransformer': {'R2': tab_r2, 'RMSE': tab_rmse},\n",
    "    'FT-Transformer': {'R2': ft_r2, 'RMSE': ft_rmse},\n",
    "    'SAINT': {'R2': saint_r2, 'RMSE': saint_rmse},\n",
    "    'TabNet': {'R2': tabnet_r2, 'RMSE': tabnet_rmse},\n",
    "    'RTDL ResNet': {'R2': rtdl_r2, 'RMSE': rtdl_rmse},\n",
    "    'AutoGluon': {'R2': autogluon_r2, 'RMSE': autogluon_rmse},\n",
    "    'Hybrid (Trans+XGB)': {'R2': hybrid_r2, 'RMSE': hybrid_rmse}\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('R2', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🏆 최신 Transformer 모델 성능 비교\")\n",
    "print(\"=\"*70)\n",
    "print(results_df)\n",
    "\n",
    "# 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# R² Score\n",
    "axes[0].barh(results_df.index, results_df['R2'], color='skyblue')\n",
    "axes[0].set_xlabel('R² Score')\n",
    "axes[0].set_title('Model Performance (R²)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1].barh(results_df.index, results_df['RMSE'], color='coral')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_title('Model Error (RMSE)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 최고 모델\n",
    "best_model = results_df.index[0]\n",
    "best_r2 = results_df.iloc[0]['R2']\n",
    "best_rmse = results_df.iloc[0]['RMSE']\n",
    "\n",
    "print(f\"\\n🥇 최고 성능 모델: {best_model}\")\n",
    "print(f\"   R² Score: {best_r2:.4f}\")\n",
    "print(f\"   RMSE: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 모델 해석 및 결론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"📊 최신 Transformer 모델 분석 결론\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n🔬 모델별 특징:\")\n",
    "print(\"\\n1. TabTransformer (Google)\")\n",
    "print(\"   - Transformer를 tabular data에 적용한 선구자\")\n",
    "print(\"   - 범주형 변수에 강점\")\n",
    "\n",
    "print(\"\\n2. FT-Transformer\")\n",
    "print(\"   - 각 특성을 토큰으로 변환\")\n",
    "print(\"   - 특성 간 복잡한 상호작용 포착\")\n",
    "\n",
    "print(\"\\n3. SAINT\")\n",
    "print(\"   - Row & Column attention\")\n",
    "print(\"   - 샘플 간 관계까지 학습\")\n",
    "\n",
    "print(\"\\n4. TabNet (Google Cloud)\")\n",
    "print(\"   - 해석 가능한 특성 선택\")\n",
    "print(\"   - 전처리 불필요\")\n",
    "\n",
    "print(\"\\n5. AutoGluon (AWS)\")\n",
    "print(\"   - 자동 앙상블\")\n",
    "print(\"   - 실무에서 즉시 사용 가능\")\n",
    "\n",
    "print(\"\\n6. Hybrid Model\")\n",
    "print(\"   - Transformer + XGBoost 결합\")\n",
    "print(\"   - 딥러닝과 부스팅의 장점 결합\")\n",
    "\n",
    "print(\"\\n💡 추천:\")\n",
    "if best_r2 > 0.5:\n",
    "    print(f\"   ✅ {best_model} 사용 권장\")\n",
    "    print(f\"   - 예측 정확도: {best_r2*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"   ⚠️ 추가 특성 엔지니어링 필요\")\n",
    "    print(f\"   - 현재 최고 성능: {best_r2*100:.1f}%\")\n",
    "\n",
    "print(\"\\n📈 성능 개선 방안:\")\n",
    "print(\"   1. 더 많은 특성 추가\")\n",
    "print(\"   2. 시계열 특성 활용\")\n",
    "print(\"   3. 도메인 지식 기반 특성 엔지니어링\")\n",
    "print(\"   4. 더 큰 모델 & 더 긴 학습\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}