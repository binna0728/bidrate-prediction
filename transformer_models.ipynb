{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤— ìµœì‹  Transformer & Foundation Models for Tabular Data\n",
    "\n",
    "## ì‚¬ìš© ëª¨ë¸:\n",
    "1. **TabTransformer** - Google Research\n",
    "2. **FT-Transformer** - Feature Tokenizer Transformer\n",
    "3. **SAINT** - Self-Attention and Intersample Attention\n",
    "4. **TabNet** - Google Cloud AI\n",
    "5. **AutoGluon-Tabular** - AWS AutoML\n",
    "6. **Hugging Face TabularTransformer**\n",
    "7. **XGBoost with Transformers** - Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer ê¸°ë°˜ Tabular ëª¨ë¸\n",
    "!pip install transformers datasets accelerate -q\n",
    "!pip install pytorch-tabnet -q\n",
    "!pip install autogluon.tabular -q\n",
    "!pip install tab-transformer-pytorch -q\n",
    "!pip install rtdl -q  # Revisiting Deep Learning for Tabular Data\n",
    "!pip install saint-pytorch -q\n",
    "!pip install torch torchvision -q\n",
    "!pip install einops -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformer models\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tab_transformer_pytorch import TabTransformer, FTTransformer\n",
    "from saint_pytorch import SAINT\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# AutoML\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# RTDL (Revisiting Deep Learning)\n",
    "import rtdl\n",
    "\n",
    "# ê¸°ë³¸ ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¡œë“œ\n",
    "df = pd.read_excel('ê°•ì›ë„ë°ê²½ê¸°ì¼ë¶€.xlsx')\n",
    "print(f\"ë°ì´í„° í¬ê¸°: {df.shape}\")\n",
    "\n",
    "# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ì„¤ì •\n",
    "X_columns = df.columns[[3, 4, 5]]  # 100ë²ˆì´ë™, 10ë²ˆì´ë™, 3ë²ˆì´ë™\n",
    "y_column = df.columns[1]  # ì‚¬ì •ìœ¨\n",
    "\n",
    "# ë°ì´í„° ì •ì œ\n",
    "data = df[list(X_columns) + [y_column]].dropna()\n",
    "X = data[list(X_columns)].values\n",
    "y = data[y_column].values\n",
    "\n",
    "# ë°ì´í„° ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ìŠ¤ì¼€ì¼ë§\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {X_train.shape}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {X_test.shape}\")\n",
    "print(f\"íŠ¹ì„± ê°œìˆ˜: {X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TabTransformer (Google Research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabTransformer ëª¨ë¸\n",
    "tab_transformer = TabTransformer(\n",
    "    categories=(),  # ì—°ì†í˜• ë³€ìˆ˜ë§Œ ì‚¬ìš©\n",
    "    num_continuous=3,  # ì—°ì†í˜• ë³€ìˆ˜ 3ê°œ\n",
    "    dim=32,\n",
    "    dim_out=1,  # íšŒê·€ ì¶œë ¥\n",
    "    depth=6,  # Transformer ì¸µ ìˆ˜\n",
    "    heads=8,  # Attention heads\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.1,\n",
    "    mlp_hidden_mults=(4, 2),\n",
    "    mlp_act=nn.ReLU(),\n",
    ").to(device)\n",
    "\n",
    "print(\"TabTransformer ì•„í‚¤í…ì²˜:\")\n",
    "print(f\"  - Transformer Depth: 6\")\n",
    "print(f\"  - Attention Heads: 8\")\n",
    "print(f\"  - Hidden Dimension: 32\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in tab_transformer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabTransformer í•™ìŠµ\n",
    "def train_transformer_model(model, X_train, y_train, X_test, y_test, epochs=100):\n",
    "    # í…ì„œ ë³€í™˜\n",
    "    X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).unsqueeze(1).to(device)\n",
    "    \n",
    "    # ì˜µí‹°ë§ˆì´ì €ì™€ ì†ì‹¤ í•¨ìˆ˜\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    # í•™ìŠµ\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # TabTransformerëŠ” ë²”ì£¼í˜•ê³¼ ì—°ì†í˜•ì„ ë¶„ë¦¬í•´ì„œ ì…ë ¥\n",
    "        outputs = model(None, X_train_tensor)  # (categories, continuous)\n",
    "        loss = criterion(outputs, y_train_tensor)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        # Eval\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_outputs = model(None, X_test_tensor)\n",
    "                test_loss = criterion(test_outputs, y_test_tensor)\n",
    "                test_losses.append(test_loss.item())\n",
    "                \n",
    "                if epoch % 20 == 0:\n",
    "                    r2 = r2_score(y_test_tensor.cpu(), test_outputs.cpu())\n",
    "                    print(f\"Epoch {epoch}: Train Loss={loss.item():.4f}, Test RÂ²={r2:.4f}\")\n",
    "    \n",
    "    # ìµœì¢… í‰ê°€\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(None, X_test_tensor).cpu().numpy()\n",
    "        r2 = r2_score(y_test, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    \n",
    "    return model, r2, rmse, train_losses\n",
    "\n",
    "# í•™ìŠµ ì‹¤í–‰\n",
    "tab_transformer_trained, tab_r2, tab_rmse, tab_losses = train_transformer_model(\n",
    "    tab_transformer, X_train_scaled, y_train, X_test_scaled, y_test\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… TabTransformer ìµœì¢… ì„±ëŠ¥: RÂ²={tab_r2:.4f}, RMSE={tab_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FT-Transformer (Feature Tokenizer Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT-Transformer\n",
    "ft_transformer = FTTransformer(\n",
    "    categories=(),  # ë²”ì£¼í˜• ì—†ìŒ\n",
    "    num_continuous=3,\n",
    "    dim=32,\n",
    "    dim_out=1,\n",
    "    depth=3,\n",
    "    heads=8,\n",
    "    attn_dropout=0.1,\n",
    "    ff_dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(\"FT-Transformer (Feature Tokenizer):\")\n",
    "print(\"  - ê° íŠ¹ì„±ì„ ê°œë³„ í† í°ìœ¼ë¡œ ë³€í™˜\")\n",
    "print(\"  - Self-attentionìœ¼ë¡œ íŠ¹ì„± ê°„ ìƒí˜¸ì‘ìš© í•™ìŠµ\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in ft_transformer.parameters()):,}\")\n",
    "\n",
    "# FT-Transformer í•™ìŠµ\n",
    "ft_transformer_trained, ft_r2, ft_rmse, ft_losses = train_transformer_model(\n",
    "    ft_transformer, X_train_scaled, y_train, X_test_scaled, y_test, epochs=100\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… FT-Transformer ìµœì¢… ì„±ëŠ¥: RÂ²={ft_r2:.4f}, RMSE={ft_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SAINT (Self-Attention and Intersample Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAINT ëª¨ë¸ - Row & Column Attention\n",
    "saint_model = SAINT(\n",
    "    num_features=3,\n",
    "    num_classes=1,  # íšŒê·€ì´ë¯€ë¡œ 1\n",
    "    dim=32,\n",
    "    depth=6,\n",
    "    heads=8,\n",
    "    dim_head=16,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(\"SAINT (Self-Attention & Intersample):\")\n",
    "print(\"  - Row-wise attention: ìƒ˜í”Œ ê°„ ê´€ê³„ í•™ìŠµ\")\n",
    "print(\"  - Column-wise attention: íŠ¹ì„± ê°„ ê´€ê³„ í•™ìŠµ\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in saint_model.parameters()):,}\")\n",
    "\n",
    "# SAINTëŠ” ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(\n",
    "    torch.FloatTensor(X_train_scaled),\n",
    "    torch.FloatTensor(y_train)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# SAINT í•™ìŠµ\n",
    "optimizer = optim.AdamW(saint_model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "saint_model.train()\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = saint_model(batch_X).squeeze()\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# SAINT í‰ê°€\n",
    "saint_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "    predictions = saint_model(X_test_tensor).cpu().numpy().squeeze()\n",
    "    saint_r2 = r2_score(y_test, predictions)\n",
    "    saint_rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print(f\"\\nâœ… SAINT ìµœì¢… ì„±ëŠ¥: RÂ²={saint_r2:.4f}, RMSE={saint_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TabNet (Google Cloud AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TabNet - Attention ê¸°ë°˜ íŠ¹ì„± ì„ íƒ\n",
    "tabnet_model = TabNetRegressor(\n",
    "    n_d=8,  # Width of decision prediction layer\n",
    "    n_a=8,  # Width of attention embedding\n",
    "    n_steps=3,  # Number of steps\n",
    "    gamma=1.3,  # Relaxation parameter\n",
    "    cat_idxs=[],  # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ë±ìŠ¤ (ì—†ìŒ)\n",
    "    cat_dims=[],  # ë²”ì£¼í˜• ë³€ìˆ˜ ì°¨ì› (ì—†ìŒ)\n",
    "    cat_emb_dim=1,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    epsilon=1e-15,\n",
    "    momentum=0.02,\n",
    "    lambda_sparse=1e-3,  # Sparsity\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=0.02),\n",
    "    scheduler_params={\"step_size\": 50, \"gamma\": 0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',  # \"sparsemax\" or \"entmax\"\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"TabNet (Explainable DL):\")\n",
    "print(\"  - Sequential attention for feature selection\")\n",
    "print(\"  - Interpretable feature importance\")\n",
    "print(\"  - No preprocessing required\")\n",
    "\n",
    "# TabNet í•™ìŠµ\n",
    "tabnet_model.fit(\n",
    "    X_train=X_train_scaled,\n",
    "    y_train=y_train.reshape(-1, 1),\n",
    "    eval_set=[(X_test_scaled, y_test.reshape(-1, 1))],\n",
    "    eval_metric=['rmse'],\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128\n",
    ")\n",
    "\n",
    "# TabNet í‰ê°€\n",
    "tabnet_pred = tabnet_model.predict(X_test_scaled).flatten()\n",
    "tabnet_r2 = r2_score(y_test, tabnet_pred)\n",
    "tabnet_rmse = np.sqrt(mean_squared_error(y_test, tabnet_pred))\n",
    "\n",
    "print(f\"\\nâœ… TabNet ìµœì¢… ì„±ëŠ¥: RÂ²={tabnet_r2:.4f}, RMSE={tabnet_rmse:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = tabnet_model.feature_importances_\n",
    "print(\"\\nTabNet Feature Importances:\")\n",
    "for i, imp in enumerate(feature_importances):\n",
    "    print(f\"  Feature {i}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. RTDL Models (Revisiting Deep Learning for Tabular Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RTDL - ResNet for Tabular\n",
    "rtdl_resnet = rtdl.ResNet.make_baseline(\n",
    "    d_in=3,  # ì…ë ¥ ì°¨ì›\n",
    "    d_out=1,  # ì¶œë ¥ ì°¨ì› (íšŒê·€)\n",
    "    n_blocks=2,\n",
    "    d_main=128,\n",
    "    d_hidden=256,\n",
    "    dropout_first=0.2,\n",
    "    dropout_second=0.1,\n",
    ").to(device)\n",
    "\n",
    "print(\"RTDL ResNet for Tabular:\")\n",
    "print(\"  - Residual connections for tabular data\")\n",
    "print(\"  - Deep architecture without degradation\")\n",
    "print(f\"  - Parameters: {sum(p.numel() for p in rtdl_resnet.parameters()):,}\")\n",
    "\n",
    "# ResNet í•™ìŠµ\n",
    "optimizer = optim.AdamW(rtdl_resnet.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "rtdl_resnet.train()\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rtdl_resnet(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss={loss.item():.4f}\")\n",
    "\n",
    "# ResNet í‰ê°€\n",
    "rtdl_resnet.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = rtdl_resnet(X_test_tensor).cpu().numpy().squeeze()\n",
    "    rtdl_r2 = r2_score(y_test, predictions)\n",
    "    rtdl_rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "\n",
    "print(f\"\\nâœ… RTDL ResNet ìµœì¢… ì„±ëŠ¥: RÂ²={rtdl_r2:.4f}, RMSE={rtdl_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. AutoGluon (AWS AutoML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoGluon - ìë™ ì•™ìƒë¸”\n",
    "train_data = pd.DataFrame(X_train, columns=['feat_0', 'feat_1', 'feat_2'])\n",
    "train_data['target'] = y_train\n",
    "test_data = pd.DataFrame(X_test, columns=['feat_0', 'feat_1', 'feat_2'])\n",
    "\n",
    "print(\"AutoGluon TabularPredictor:\")\n",
    "print(\"  - Automatic model selection & ensembling\")\n",
    "print(\"  - Neural networks + Boosted trees + More\")\n",
    "print(\"  - Automatic hyperparameter tuning\")\n",
    "\n",
    "# AutoGluon í•™ìŠµ\n",
    "predictor = TabularPredictor(\n",
    "    label='target',\n",
    "    eval_metric='r2',\n",
    "    verbosity=0\n",
    ").fit(\n",
    "    train_data=train_data,\n",
    "    presets='best_quality',  # ìµœê³  í’ˆì§ˆ ì„¤ì •\n",
    "    time_limit=120  # 2ë¶„ ì œí•œ\n",
    ")\n",
    "\n",
    "# AutoGluon í‰ê°€\n",
    "autogluon_pred = predictor.predict(test_data)\n",
    "autogluon_r2 = r2_score(y_test, autogluon_pred)\n",
    "autogluon_rmse = np.sqrt(mean_squared_error(y_test, autogluon_pred))\n",
    "\n",
    "print(f\"\\nâœ… AutoGluon ìµœì¢… ì„±ëŠ¥: RÂ²={autogluon_r2:.4f}, RMSE={autogluon_rmse:.4f}\")\n",
    "\n",
    "# ëª¨ë¸ ë¦¬ë”ë³´ë“œ\n",
    "print(\"\\nAutoGluon Model Leaderboard:\")\n",
    "leaderboard = predictor.leaderboard(test_data)\n",
    "print(leaderboard[['model', 'score_val', 'score_test']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Hybrid: XGBoost + Transformer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Transformerë¡œ íŠ¹ì„± ì¶”ì¶œ í›„ XGBoost\n",
    "print(\"Hybrid Model: Transformer Feature Extractor + XGBoost\")\n",
    "print(\"  - Transformerë¡œ ê³ ì°¨ì› íŠ¹ì„± í•™ìŠµ\")\n",
    "print(\"  - XGBoostë¡œ ìµœì¢… ì˜ˆì¸¡\")\n",
    "\n",
    "# TabTransformerì—ì„œ íŠ¹ì„± ì¶”ì¶œ (ë§ˆì§€ë§‰ ì¸µ ì „)\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, transformer_model):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transformerì˜ ì¤‘ê°„ representation ì¶”ì¶œ\n",
    "        with torch.no_grad():\n",
    "            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì›ë³¸ íŠ¹ì„±ê³¼ ë³€í™˜ëœ íŠ¹ì„±ì„ ì—°ê²°\n",
    "            transformed = self.transformer(None, x)\n",
    "        return torch.cat([x, transformed], dim=1)\n",
    "\n",
    "# íŠ¹ì„± ì¶”ì¶œ\n",
    "extractor = FeatureExtractor(tab_transformer_trained).to(device)\n",
    "extractor.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_train_enhanced = extractor(torch.FloatTensor(X_train_scaled).to(device)).cpu().numpy()\n",
    "    X_test_enhanced = extractor(torch.FloatTensor(X_test_scaled).to(device)).cpu().numpy()\n",
    "\n",
    "print(f\"Enhanced features shape: {X_train_enhanced.shape}\")\n",
    "\n",
    "# XGBoost with enhanced features\n",
    "xgb_hybrid = xgb.XGBRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_hybrid.fit(X_train_enhanced, y_train)\n",
    "hybrid_pred = xgb_hybrid.predict(X_test_enhanced)\n",
    "hybrid_r2 = r2_score(y_test, hybrid_pred)\n",
    "hybrid_rmse = np.sqrt(mean_squared_error(y_test, hybrid_pred))\n",
    "\n",
    "print(f\"\\nâœ… Hybrid (Transformer+XGBoost) ìµœì¢… ì„±ëŠ¥: RÂ²={hybrid_r2:.4f}, RMSE={hybrid_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ëª¨ë“  ëª¨ë¸ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì •ë¦¬\n",
    "results = {\n",
    "    'TabTransformer': {'R2': tab_r2, 'RMSE': tab_rmse},\n",
    "    'FT-Transformer': {'R2': ft_r2, 'RMSE': ft_rmse},\n",
    "    'SAINT': {'R2': saint_r2, 'RMSE': saint_rmse},\n",
    "    'TabNet': {'R2': tabnet_r2, 'RMSE': tabnet_rmse},\n",
    "    'RTDL ResNet': {'R2': rtdl_r2, 'RMSE': rtdl_rmse},\n",
    "    'AutoGluon': {'R2': autogluon_r2, 'RMSE': autogluon_rmse},\n",
    "    'Hybrid (Trans+XGB)': {'R2': hybrid_r2, 'RMSE': hybrid_rmse}\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df = results_df.sort_values('R2', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ† ìµœì‹  Transformer ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\")\n",
    "print(\"=\"*70)\n",
    "print(results_df)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# RÂ² Score\n",
    "axes[0].barh(results_df.index, results_df['R2'], color='skyblue')\n",
    "axes[0].set_xlabel('RÂ² Score')\n",
    "axes[0].set_title('Model Performance (RÂ²)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# RMSE\n",
    "axes[1].barh(results_df.index, results_df['RMSE'], color='coral')\n",
    "axes[1].set_xlabel('RMSE')\n",
    "axes[1].set_title('Model Error (RMSE)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ìµœê³  ëª¨ë¸\n",
    "best_model = results_df.index[0]\n",
    "best_r2 = results_df.iloc[0]['R2']\n",
    "best_rmse = results_df.iloc[0]['RMSE']\n",
    "\n",
    "print(f\"\\nğŸ¥‡ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}\")\n",
    "print(f\"   RÂ² Score: {best_r2:.4f}\")\n",
    "print(f\"   RMSE: {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ëª¨ë¸ í•´ì„ ë° ê²°ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"ğŸ“Š ìµœì‹  Transformer ëª¨ë¸ ë¶„ì„ ê²°ë¡ \")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ”¬ ëª¨ë¸ë³„ íŠ¹ì§•:\")\n",
    "print(\"\\n1. TabTransformer (Google)\")\n",
    "print(\"   - Transformerë¥¼ tabular dataì— ì ìš©í•œ ì„ êµ¬ì\")\n",
    "print(\"   - ë²”ì£¼í˜• ë³€ìˆ˜ì— ê°•ì \")\n",
    "\n",
    "print(\"\\n2. FT-Transformer\")\n",
    "print(\"   - ê° íŠ¹ì„±ì„ í† í°ìœ¼ë¡œ ë³€í™˜\")\n",
    "print(\"   - íŠ¹ì„± ê°„ ë³µì¡í•œ ìƒí˜¸ì‘ìš© í¬ì°©\")\n",
    "\n",
    "print(\"\\n3. SAINT\")\n",
    "print(\"   - Row & Column attention\")\n",
    "print(\"   - ìƒ˜í”Œ ê°„ ê´€ê³„ê¹Œì§€ í•™ìŠµ\")\n",
    "\n",
    "print(\"\\n4. TabNet (Google Cloud)\")\n",
    "print(\"   - í•´ì„ ê°€ëŠ¥í•œ íŠ¹ì„± ì„ íƒ\")\n",
    "print(\"   - ì „ì²˜ë¦¬ ë¶ˆí•„ìš”\")\n",
    "\n",
    "print(\"\\n5. AutoGluon (AWS)\")\n",
    "print(\"   - ìë™ ì•™ìƒë¸”\")\n",
    "print(\"   - ì‹¤ë¬´ì—ì„œ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥\")\n",
    "\n",
    "print(\"\\n6. Hybrid Model\")\n",
    "print(\"   - Transformer + XGBoost ê²°í•©\")\n",
    "print(\"   - ë”¥ëŸ¬ë‹ê³¼ ë¶€ìŠ¤íŒ…ì˜ ì¥ì  ê²°í•©\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ì¶”ì²œ:\")\n",
    "if best_r2 > 0.5:\n",
    "    print(f\"   âœ… {best_model} ì‚¬ìš© ê¶Œì¥\")\n",
    "    print(f\"   - ì˜ˆì¸¡ ì •í™•ë„: {best_r2*100:.1f}%\")\n",
    "else:\n",
    "    print(f\"   âš ï¸ ì¶”ê°€ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í•„ìš”\")\n",
    "    print(f\"   - í˜„ì¬ ìµœê³  ì„±ëŠ¥: {best_r2*100:.1f}%\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ ì„±ëŠ¥ ê°œì„  ë°©ì•ˆ:\")\n",
    "print(\"   1. ë” ë§ì€ íŠ¹ì„± ì¶”ê°€\")\n",
    "print(\"   2. ì‹œê³„ì—´ íŠ¹ì„± í™œìš©\")\n",
    "print(\"   3. ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§\")\n",
    "print(\"   4. ë” í° ëª¨ë¸ & ë” ê¸´ í•™ìŠµ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}